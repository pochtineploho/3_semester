#### Стандартное дискретное распределение
1. Распределение Бернулли $B_{p}, \, 0 < p < 1$
	$\xi$ - число успехов при одном испытании, где $p$ - вероятность успеха
	
| <!-- --> | <!-- --> | <!-- --> |
|:--------:|:--------:|:--------:|
|  $\xi$   |    0     |    1     |
|    p     |   1-p    |    p     |

$E \xi = 0 (1-p) + 1 \cdot p = p$
$D\xi = 0^2(1-p) + 1^2 p - p ^ 2 = pq$
$I_{A} \in B_{p}$, где $I_{A} = \begin{cases} 0, \, \text{A - не произошло} \\ 1, \, \text{A - произошло}\ \end{cases}$

2. Биномиальное распределение $B_{n, p}$
	$\xi$ - число успехов при n испытаниях, где p - вероятность при одном испытании
	$\xi \in B_{n,p} \Longleftrightarrow P(\xi = k) = C^k_{n} \cdot p^k q^{n-k}, \, 0 \leq k \leq n$
	Note: $B_{p} = B_{1,p}$

| <!-- --> | <!-- --> | <!-- --> |          <!-- -->           | <!-- --> |          <!-- -->           | <!-- -->               | <!-- --> |  
|:--------:|:--------:|:--------:|:---------------------------:|:--------:|:------------:|:-------:|:--------:|
|  $\xi$   |    0     |    1     |              2              | $\dots$  |              k              | $\dots$ |    n     | 
|    p     |   1-p    |    p     | $C^2_{n} \cdot p^2 q^{n-2}$ | $\dots$  | $C^k_{n} \cdot p^k q^{n-k}$ | $\dots$ |  $p^n$   |  

Заметим, что $\xi = \xi_{1}+ \dots + \xi_{n}$, где $\xi_{i} \in B_{p}$ - число успехов при i-том испытании (независимы)
$E \xi_{i} = p; D \xi_{i} = pq$
$E \xi = E \xi_{1} + E \xi_{2} + \dots + E \xi_{n} = p + p + \dots = np$
$D \xi = D \xi_{1} + D \xi_{2} + \dots + D \xi_{n} = \underbrace{pq+pq+\dots+pq}_{n} = npq$
$\sigma = \sqrt{ D \delta } = \sqrt{ npq }$
3. Геометрическое распределение $G_{p}, \, 0 < p < 1$
	$\xi$ - номер первого успеха в серии независимых испытаний, где p - вероятность успеха при одном испытании
	$P(\delta = k) = q^{k-1}p, \, k=1,2,3,\dots$

| <!-- --> | <!-- --> | <!-- --> |          <!-- -->           | <!-- --> |          <!-- -->           |   
|:--------:|:--------:|:--------:|:---------------:|:--------:|:---------------:|:-------:|
|  $\xi$   |    1     |    2     |              3              | $\dots$  |              k              | $\dots$  | 
|    p     |   p    |    qp     | $q^2 p$ | $\dots$  | $q^{k-1}p$ | $\dots$  |  

$E \xi = \sum\limits_{k=0}^{\infty}kq^{k-1}p = p\sum\limits_{k=1}^{\infty}kq^{k-1} = p\sum\limits_{k=1}^{\infty}(q^{k})^{'} = p \left( \sum\limits_{k=1}^{\infty}q^{k} \right)^{'} = p\left( \frac{1}{1-q} \right)'=p\frac{1}{(1-q)^2} = \frac{p}{p^2} = \frac{1}{p}$ 
$E \xi^2 = \sum\limits_{k=1}^{\infty}k^2 q^{k-1}p = \sum\limits_{k=1}^{\infty}(k(k-1) + k)q^{k-1}p = p\sum\limits_{k=1}^{\infty}k(k-1)q^{k-1} + p\sum\limits_{k=1}^{\infty}kq^{k-1} = pq\sum\limits_{k=1}^{\infty}k(k-1)q^{k-2} + E\delta =$
$= pq \sum\limits_{k=1}^{\infty}(q^k)^{''} + \frac{1}{p} = pq\left( \frac{1}{1-q} \right)'' + \frac{1}{p} = 2pq \frac{1}{(1-q)^2} + \frac{1}{p} = \frac{2q}{p^2} + \frac{1}{p}$
$D\delta = E\delta^2-(E\delta)^2 = \frac{2q}{p^2}+\frac{1}{p}-\frac{1}{p}^2= \frac{2q+p-1}{p^2} = \frac{q}{p^2}$

4. Распределение Пуассона $П_{\lambda}$ (с параметром $\lambda$ > 0)
Определение. Случайна величина $\delta \in П_{\lambda},$ если $P(\delta = k) = \frac{\lambda^k}{k!}e^{-\lambda},\, k = 0,1,2,\dots$

| <!-- --> |    <!-- -->    |        <!-- -->        |             <!-- -->              | <!-- --> |              <!-- -->              | <!-- --> |
|:--------:|:--------------:|:----------------------:|:---------------------------------:|:--------:|:----------------------------------:|:--------:|
|  $\xi$   |       0        |           1            |                 2                 | $\dots$  |                 k                  | $\dots$  |
|    p     | $e^{-\lambda}$ | $\lambda e^{-\lambda}$ | $\frac{\lambda^2}{2}e^{-\lambda}$ | $\dots$  | $\frac{\lambda^k}{k!}e^{-\lambda}$ | $\dots$  |

$\sum\limits_{k=0}^{\infty} p_k= \sum\limits_{k=0}^{\infty} \frac{\lambda^k}{k!}e^{-\lambda} = e^{-\lambda} \sum\limits_{k=0}^{\infty} \frac{\lambda^k}{k!} = e^{-\lambda} e^{\lambda} = 1$
$E\delta = \sum\limits_{k=0}^{\infty}k \frac{\lambda^k}{k!}e^{-\lambda} = e^{-\lambda} \sum\limits_{k=1}^{\infty} \frac{\lambda^k}{(k-1)!} = \lambda e^{-\lambda} \sum\limits_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} = e^{-\lambda} \sum\limits_{k=0}^{\infty} \frac{\lambda^k}{k!} = \lambda e^{-\lambda} e^\lambda = \lambda$
$E\delta^2=\sum\limits_{k=0}^{\infty} k^2 \frac{\lambda^k}{k!} e^{-\lambda} = \sum\limits_{k=0}^{\infty} (k(k-1)+k) \frac{\lambda^k}{k!} e^{-\lambda} = e^{-\lambda}\sum\limits_{k=2}^{\infty} k(k-1) \frac{\lambda^k}{k!} + e^{-\lambda}\sum\limits_{k=1}^{\infty} k \frac{\lambda^k}{k!} = e^{-\lambda}\lambda^2 \sum\limits_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!} + E\delta =$
$=e^{-\lambda}\lambda^2 \sum\limits_{k=0}^{\infty}\frac{\lambda^{k}}{k!} + \lambda = \lambda^2 + \lambda$
$D\delta = E\delta^2-(E\delta)^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$

5. Целочисленные случайные величины и их производящие функции
	Пусть имеются целочисленные случайные величины с законом распределения
	$P(\delta = k) = p_{k}, \, k = 0,1,2,\dots$
	Для последовательности вероятности введём производящие функции 
	$P(S) = p_{0} + p_{1}S + p_{2}S^2 + \dots + p_{k}S^k + \dots = \sum\limits_{k=0}^{\infty}p_{k}S^k$
	Т.к. $0 \leq p_{k} \leq 1$ - ограничено, то ряд сходится при $|S|<1$
	при $S = 1 : \sum\limits_{k=0}^{\infty}p_{k}=1$ - сходится $\implies$ ряд сходится при $|S|\leq 1$
	Теорема 1. $E \xi = p'(1)$ при условии, что $E \xi$ существует
		$p'(S) = \left( \sum\limits_{k=0}^{\infty} p_{k} S^k \right)' = \sum\limits_{k=0}^{\infty} k p_{k} S^{k-1}$ 
		при $S = 1$ $\sum\limits_{k=0}^{\infty} k p_{k} = E \xi$
	Теорема 2. $D = p''(1) + p'(1)-(p'(1))^2$
		$p''(S) = (\sum\limits_{k=0}^{\infty} p_{k} S^k)'' = \sum\limits_{k=0}^{\infty} k(k-1) p_{k} S^{k-2}$
		$p''(1) = \sum\limits_{k=0}^{\infty} k(k-1) p_{k} =  \sum\limits_{k=0}^{\infty} k^2 p_{k} -  \sum\limits_{k=0}^{\infty} k p_{k} = E \xi^2 - E \xi$
		$D \xi = E \xi^2 - (E \xi)^2 =p''(1)+p'(1) -(p'(1))^2$

6. Свёртка (композиция)
Пусть $\xi, \eta$ - независимые случайные величины (целочисленные) с производящими функциями $A(S)$ и $B(S)$, $p_{k} = P(\xi = k)$, $q_{k} = P(\eta = k)$. Тогда $S = \xi + \eta$ - также целочисленная случайная величина, закон распределения которой $p(S = k) = C_{k}$, где $C_{k} = P(\xi = 0, \eta = k) + P(\xi = 1, \eta = k-1) + P(\xi = 2, \eta = k-2) + \dots + P(\xi = k, \eta = 0) =$
$=p_{0}q_{k} + p_{1}q_{k-1}+ \dots + p_{k}q_{0}$
$A(S) = \sum\limits_{k=0}^{\infty}p_{k}S^k$   $B(S) = \sum\limits_{k=0}^{\infty} =q_{k}S^k$
$C_{k}$ - коэффициент при $S^k$ в произведении $A(S)B(S)$
Получили теорему: производная функция суммы независимых случайных величин равна произведению их производящих функций
$C(S) = A(S)B(S)$
Определение: операция над двумя послед. $\{p_{k}\}$ и $\{q_{k}\}$ называется и свёрткой и обозначается $\{C_{k}\} = \{p_{k}\}\cdot\{q_{k}\}$
Следствие: производящая функция суммы n независимых случайных величин с производящей функцией $p(S)$ равна $p^n(S)$
Определение: распределение устойчиво относительно суммирования, если сумма независимых друг от друга величин также будет иметь данное распределение.
Note: Так как целочисленные случайные величины полностью задаются своими производящими функциями, то их произведение устойчиво.

Примеры:
1) Распределение Бернулли $B_{p}$

| <!-- --> | <!-- --> | <!-- --> |
|:--------:|:--------:|:--------:|
|  $\xi$   |    0     |    1     |
| p         |  1-p        |   p       |

$P(S) = 1 - p + pS = q + pS$
2) Биномиальное распределение
$P(S) = \sum\limits_{k=0}^{n}C^k_{n}p^kq^{n-k}S^k = \sum\limits_{k=0}^{n}C^k_{n}(pS)^k q^{n-k} = (q + pS)^n$
Найдём матожидание и дисперсию через произведение функций
$P'(S) = n(q+pS)^{n-1}p$
$P''(S) = np(n-1)(q + pS)^{n-2}p = n(n-1)p^2(q-pS)^{n-2}$
$E \xi = p'(1) = n(q + p)^{n-1}p = np$
$p''(1) = n(n-1)p^2(q + pS)^{n-2} = n(n-1)p^2(q+p)^{n-2} = n(n-1)p^2$
$D \xi = p''(1) + p'(1) - (p'(1))^2 = n(n-1)p^2 + np - n^2p^2 = n^2p^2-np^2+np - n^2p^2 = npq$

Проверим устойчивость $B_{n,p}$
$] \xi = B_{n,p}$ и $\eta \in B_{n,p}$ - независимая случайная величина
$A(S) = (q+pS)^n$ и $B(S) = (q+pS)^m$
Производящая функция $\eta + \xi$:   $A(S)B(S) = (q+pS)^n(q+pS)^m=(q+pS)^{n+m}$
Получили производящую функцию $B_{n+m,p}$. Устойчиво относительно суммирования

#### Элементы теории принятия решений
Пусть требуется принять одно из возможных решений $A_{1}, A_{2}, \dots, A_{m}$. Результат принятия данного решения зависит от одной из возможных ситуаций $Q_{1}, Q_{2}, \dots, Q_{n}$. Обозначим $a_{ij}$ - результат при принятии $i$-того решения ($A_{i}$) и случившейся при этом ситуации $Q_{j}$
Платёжная матрица
$$A=(a_{ij}) = \begin{pmatrix}
a_{11} & a_{12} & \dots & a_{1n}  \\
\\ a_{21} & a_{22} & \dots & a_{2n} \\
\vdots   \\
a_{m1} & a_{m2} & \dots & a_{mn} 
\end{pmatrix}$$
Если вероятности $p_{i}$ возможных ситуаций $Q_{j}$ неизвестны, то говорят, что решение принимается в условиях полной неопределённости, а если известны - то в условиях риска, тогда платёжную матрицу удобно записывать:
$$A=\left( \frac{p_{i}}{a_{ij}} \right) = \begin{pmatrix}
p_{1} & p_{2} & \dots & p_{n} \\ --- & --- & --- & --- \\a_{11} & a_{12} & \dots & a_{1n}  \\
\\ a_{21} & a_{22} & \dots & a_{2n} \\
\vdots   \\
a_{m1} & a_{m2} & \dots & a_{mn} 
\end{pmatrix}$$
1. Байесовский принцип принятия решений (максимизации прибыли)
По этому принципу выбираем то решение, где средняя ожидаемая прибыль $\bar{a_{j}} = \sum\limits_{i=1}^{n} a_{ij} p_{j}$ является максимальным
2. Принцип минимизации риска.
	В качестве оптимального выбираем решение, где меньше риск (дисперсия)